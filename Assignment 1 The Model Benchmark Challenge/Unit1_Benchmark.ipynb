{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR5idZbZJi3W",
        "outputId": "7349f68c-d19d-466c-afbf-6ce5139c3508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== MUST BE AT THE TOP =====\n",
        "import os\n",
        "import warnings\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "# 1. Disable all Python warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 2. Disable Hugging Face / Transformers logs\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# 3. Disable Hugging Face progress bars (config, tokenizer, model downloads)\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
        "\n",
        "# ===== YOUR ACTUAL CODE =====\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            device=-1  # force CPU\n",
        "        )\n",
        "        output = generator(prompt, max_new_tokens=15)\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception:\n",
        "        print(\"Generation failed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaMvBalwK12g",
        "outputId": "a2ff93c9-36bc-442d-ca4c-c79c3d0c29f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n",
            "The future of Artificial Intelligence is...............\n",
            "\n",
            "Model: RoBERTa\n",
            "The future of Artificial Intelligence is\n",
            "\n",
            "Model: BART\n",
            "The future of Artificial Intelligence isinvolved TreesGUamsung272involved Trees NavimmerDEBUGDEBUGDEBUG Thin Dum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentences = {\n",
        "    \"BERT\": (\"bert-base-uncased\", \"The goal of Generative AI is to [MASK] new content.\"),\n",
        "    \"RoBERTa\": (\"roberta-base\", \"The goal of Generative AI is to <mask> new content.\"),\n",
        "    \"BART\": (\"facebook/bart-base\", \"The goal of Generative AI is to <mask> new content.\")\n",
        "}\n",
        "\n",
        "for name, (model, text) in sentences.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        masker = pipeline(\"fill-mask\", model=model)\n",
        "        results = masker(text)\n",
        "        for r in results[:3]:\n",
        "            print(r[\"token_str\"], \"-\", round(r[\"score\"], 4))\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSeHkeoAMwHO",
        "outputId": "83e3d1c7-4513-4f34-8114-8ba918fcf02d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n",
            "create - 0.5397\n",
            "generate - 0.1558\n",
            "produce - 0.0541\n",
            "\n",
            "Model: RoBERTa\n",
            " generate - 0.3711\n",
            " create - 0.3677\n",
            " discover - 0.0835\n",
            "\n",
            "Model: BART\n",
            " create - 0.0746\n",
            " help - 0.0657\n",
            " provide - 0.0609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model)\n",
        "        result = qa(question=question, context=context)\n",
        "        print(\"Answer:\", result[\"answer\"])\n",
        "        print(\"Score:\", round(result[\"score\"], 4))\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo0za-DpNt2A",
        "outputId": "10a5aed2-1230-450a-b42b-edddbbaec6e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n",
            "Answer: hallucinations,\n",
            "Score: 0.0097\n",
            "\n",
            "Model: RoBERTa\n",
            "Answer: deepfakes\n",
            "Score: 0.0121\n",
            "\n",
            "Model: BART\n",
            "Answer: significant risks such as hallucinations, bias,\n",
            "Score: 0.0483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------|-------|----------------------------------|--------------------------------------|-------------------------------------------|\n",
        "| **Generation** | BERT | Failure | The model produced repetitive symbols instead of meaningful text. | BERT is an encoder-only model and is not trained to predict the next word. |\n",
        "|  | RoBERTa | Failure | The model failed to generate any continuation beyond the prompt. | RoBERTa is also an encoder-only model without autoregressive generation capability. |\n",
        "|  | BART | Partial Success | The model generated text, but the output was incoherent and nonsensical. | BART has an encoder-decoder architecture, but the base model is not fine-tuned for open-ended text generation. |\n",
        "| **Fill-Mask** | BERT | Success | The model correctly predicted words such as “create” and “generate” with high confidence. | BERT is trained using Masked Language Modeling (MLM). |\n",
        "|  | RoBERTa | Success | The model accurately predicted suitable missing words with strong confidence. | RoBERTa is optimized for MLM with improved training strategy and data. |\n",
        "|  | BART | Partial Success | The model predicted reasonable words but with lower confidence. | BART is trained for sequence-to-sequence denoising, not primarily for MLM tasks. |\n",
        "| **QA** | BERT | Partial Failure | The model extracted a long phrase from the context but with very low confidence. | The base BERT model is not fine-tuned for Question Answering tasks such as SQuAD. |\n",
        "|  | RoBERTa | Failure | The model returned an incomplete one-word answer with very low confidence. | RoBERTa base lacks QA fine-tuning and an effective span-prediction head. |\n",
        "|  | BART | Partial Failure | The model extracted a slightly relevant but incomplete answer with low confidence. | Although BART supports seq2seq tasks, it is not trained for extractive QA in its base form. |\n"
      ],
      "metadata": {
        "id": "-a0xILcR7QiA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYEL8mfQ7xAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}